{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding = utf-8\n",
    "\n",
    "\"\"\"\n",
    "Coding the implementation of a neural network using numpy arrays\n",
    "Forward propagate\n",
    "Backward propagation\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = gzip.open(\"mnist.pkl.gz\", mode='rb')\n",
    "train, validate, test = pickle.load(f, encoding= 'latin1')\n",
    "X_train, y_train = train[0], train[1]\n",
    "length, width = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "\n",
    "    def convert_toOneHot(self,y):\n",
    "        y_ = np.zeros((len(y),10))\n",
    "        y_[np.arange(len(y)),y] = 1\n",
    "        return y_\n",
    "        \n",
    "    def __init__(self, input_data, input_layers,hidden_layers_size, output_layers):\n",
    "        # hidden_layers_size is a list of number of nodes in each hidden layer\n",
    "        self.X_train = input_data[0]\n",
    "        self.y_train = input_data[1]\n",
    "        self.y_oneHot = self.convert_toOneHot(self.y_train) # The function is above\n",
    "        self.input_layers = input_layers\n",
    "        self.hidden_layers_size = hidden_layers_size\n",
    "        self.output_layers = output_layers\n",
    "        \n",
    "        # Initializing the network\n",
    "        self.initialize_network()\n",
    "\n",
    "    def initialize_network(self):\n",
    "        self.weights = {}\n",
    "        self.delta = {}\n",
    "        self.error= {}\n",
    "\n",
    "        # We define the first and last set of weights manually and loop through to set the other weights\n",
    "\n",
    "        self.weights[0] = np.random.randn(self.input_layers + 1,self.hidden_layers_size[0])\n",
    "        # The seld.input_layers + 1 is for the bias term\n",
    "        \n",
    "        self.weights[len(self.hidden_layers_size)] = np.random.randn(self.hidden_layers_size[-1],self.output_layers)\n",
    "        for i in range(len(self.hidden_layers_size) - 1):   # Since we already defined the last layer\n",
    "            self.weights[i+1] = np.random.randn(self.hidden_layers_size[i],self.hidden_layers_size[i+1])\n",
    "            \n",
    "        \n",
    "    def mini_batch(self,size_of_batch):\n",
    "        \"\"\"\n",
    "        Chooses a number of random examples from the whole training set depending on the\n",
    "        size_of_batch specified\n",
    "        \"\"\"\n",
    "        idx = np.random.choice(len(X_train),size = size_of_batch, replace = False)\n",
    "        \"\"\"\n",
    "        The below step adds the bias to the training set and bias in our\n",
    "        example is only used in the input layer\n",
    "        \"\"\"\n",
    "        self.X_train_batch = np.hstack((X_train[idx,:],np.ones((size_of_batch,1)))) \n",
    "        self.y_train_batch = self.y_oneHot[idx]\n",
    "        \n",
    "    def compute_product(self,x,w):\n",
    "        return np.dot(x,w)\n",
    "    \n",
    "    def calc_activation(self,x,w):\n",
    "        \"\"\"\n",
    "        The below function is sigmoid activation function\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-(self.compute_product(x,w)))) # np.dot(x,w) = (x1w1 + x2w2 + ......., it also includes bias\n",
    "    \n",
    "    \n",
    "    def softmax_layer(self,x):\n",
    "        \"\"\"\n",
    "        Insert this layer wherever desired to convert the scores into probabilities\n",
    "        \"\"\"\n",
    "        sum_exp_x = np.sum(np.exp(x.T),axis = 0)\n",
    "        return (np.divide(np.exp(x.T),sum_exp_x)).T\n",
    "    \n",
    "    def forward_propagate(self):\n",
    "        \n",
    "        self.activation = {}\n",
    "        self.mini_batch(100)\n",
    "        # Calculating the activation at the first hidden layer\n",
    "        self.activation[0] = np.copy(self.X_train_batch) # Set the activation of 0 layer as the input itself\n",
    "        \n",
    "        for i in range(len(self.hidden_layers_size) + 1):\n",
    "            self.activation[i + 1] = self.calc_activation(self.activation[i], self.weights[i])\n",
    "        \n",
    "        \"\"\"\n",
    "        For last layer we need to still compute the product to get the scores\n",
    "        we could also insert a softmax layer if need be\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # Before applying softmax\n",
    "        \n",
    "        self.activation[len(self.hidden_layers_size) + 1] = self.compute_product(self.activation[len(self.hidden_layers_size)],\n",
    "                                          self.weights[len(self.hidden_layers_size)])\n",
    "        \n",
    "        # Applying softmax \n",
    "        \n",
    "        self.activation[len(self.hidden_layers_size) + 1] = self.softmax_layer(self.activation[len(self.hidden_layers_size) + 1]) # After softmax layer\n",
    "        \n",
    "        \n",
    "    def cross_entropy(self,y,y_):\n",
    "        return np.mean(-np.sum(y*y_, axis =1), axis = 0)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        return np.mean(np.equal(np.argmax(self.y_train_batch,axis=1),\n",
    "                                np.argmax(self.activation[len(self.hidden_layers_size) + 1], axis = 1)))\n",
    "        \n",
    "            \n",
    "    \n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network(train,width,[200,100,50],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN.forward_propagate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 8, 9, 8, 8, 3, 8, 8, 3, 8, 3, 9, 3, 3, 8, 8, 8, 3, 8, 8, 3, 8, 8,\n",
       "       8, 8, 3, 8, 3, 3, 9, 8, 3, 8, 3, 3, 3, 0, 8, 8, 8, 8, 8, 8, 8, 9, 3,\n",
       "       0, 8, 8, 8, 8, 8, 9, 9, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 0, 5, 1, 9, 3,\n",
       "       9, 8, 8, 8, 3, 3, 9, 8, 9, 9, 8, 8, 8, 8, 8, 8, 3, 8, 8, 9, 3, 8, 8,\n",
       "       9, 8, 9, 8, 9, 3, 8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(NN.activation[len(NN.hidden_layers_size) + 1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 0, 1, 1, 1, 7, 9, 6, 9, 2, 8, 7, 5, 3, 3, 0, 2, 3, 7, 1, 9, 5,\n",
       "       5, 8, 8, 8, 7, 1, 8, 5, 1, 2, 1, 7, 7, 1, 8, 0, 9, 5, 7, 3, 8, 7, 1,\n",
       "       8, 5, 5, 4, 6, 4, 2, 4, 8, 2, 8, 6, 6, 8, 4, 4, 2, 8, 7, 7, 5, 2, 3,\n",
       "       6, 2, 1, 2, 8, 7, 5, 7, 2, 0, 1, 7, 0, 0, 1, 4, 7, 5, 0, 7, 7, 7, 0,\n",
       "       3, 2, 9, 2, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(NN.y_train_batch, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10679623712136026"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.cross_entropy(NN.y_train_batch, NN.activation[len(NN.hidden_layers_size) + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000000001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [carnd-term1]",
   "language": "python",
   "name": "Python [carnd-term1]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
