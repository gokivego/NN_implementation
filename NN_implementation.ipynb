{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding = utf-8\n",
    "\"\"\"\n",
    "Coding the implementation of a neural network using numpy arrays\n",
    "Forward propagate\n",
    "Backward propagation\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = gzip.open(\"mnist.pkl.gz\", mode='rb')\n",
    "train, validate, test = pickle.load(f, encoding= 'latin1')\n",
    "X_train, y_train = train[0], train[1]\n",
    "length, width = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "\n",
    "    def convert_toOneHot(self,y):\n",
    "        y_ = np.zeros((len(y),10))\n",
    "        y_[np.arange(len(y)),y] = 1\n",
    "        return y_\n",
    "        \n",
    "    def __init__(self, input_data, input_layers,hidden_layers_size, output_layers):\n",
    "        # hidden_layers_size is a list of number of nodes in each hidden layer\n",
    "        self.X_train = input_data[0]\n",
    "        self.y_train = input_data[1]\n",
    "        self.y_oneHot = self.convert_toOneHot(self.y_train) # The function is above\n",
    "        self.input_layers = input_layers\n",
    "        self.hidden_layers_size = hidden_layers_size\n",
    "        self.output_layers = output_layers\n",
    "        \n",
    "        # Initializing the network\n",
    "        self.initialize_network()\n",
    "\n",
    "    def initialize_network(self):\n",
    "        self.weights = {}\n",
    "\n",
    "        # We define the first and last set of weights manually and lopp through to set the other weights\n",
    "\n",
    "        self.weights[0] = np.random.randn(self.input_layers + 1,self.hidden_layers_size[0])\n",
    "        # The seld.input_layers + 1 is for the bias term\n",
    "        \n",
    "        self.weights[len(self.hidden_layers_size)] = np.random.randn(self.hidden_layers_size[-1],self.output_layers)\n",
    "        for i in range(len(self.hidden_layers_size) - 1):   # Since we already defined the last layer\n",
    "            self.weights[i+1] = np.random.randn(self.hidden_layers_size[i],self.hidden_layers_size[i+1])\n",
    "\n",
    "    def mini_batch(self,size_of_batch):\n",
    "        \"\"\"\n",
    "        Chooses a number of random examples from the whole training set depending on the\n",
    "        size_of_batch specified\n",
    "        \"\"\"\n",
    "        idx = np.random.choice(len(X_train),size = size_of_batch, replace = False)\n",
    "        \"\"\"\n",
    "        The below step adds the bias to the training set and bias in our\n",
    "        example is only used in the input layer\n",
    "        \"\"\"\n",
    "        self.X_train_batch = np.hstack((X_train[idx,:],np.ones((size_of_batch,1)))) \n",
    "        self.y_train_batch = self.y_oneHot[idx]\n",
    "        \n",
    "    def compute_product(self,x,w):\n",
    "        return np.dot(x,w)\n",
    "    \n",
    "    def calc_activation(self,x,w):\n",
    "        \"\"\"\n",
    "        The below function is sigmoid activation function\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-(self.compute_product(x,w)))) # np.dot(x,w) = (x1w1 + x2w2 + ......., it also includes bias\n",
    "    \n",
    "    \n",
    "    def softmax_layer(self,x):\n",
    "        \"\"\"\n",
    "        Insert this layer wherever desired to convert the scores into probabilities\n",
    "        \"\"\"\n",
    "        sum_exp_x = np.sum(np.exp(x.T),axis = 0)\n",
    "        return (np.divide(np.exp(x.T),sum_exp_x)).T\n",
    "    \n",
    "    def forward_propagate(self):\n",
    "        \n",
    "        self.activation = {}\n",
    "        self.mini_batch(100)\n",
    "        # Calculating the activation at the first hidden layer\n",
    "        self.activation[0] = np.copy(self.X_train_batch) # Set the activation of 0 layer as the input itself\n",
    "        \n",
    "        for i in range(len(self.hidden_layers_size)):\n",
    "            self.activation[i+1] = self.calc_activation(self.activation[i], self.weights[i])\n",
    "        \n",
    "        \"\"\"\n",
    "        For last layer we need to still compute the product to get the scores\n",
    "        we could also insert a softmax layer if need be\n",
    "        \n",
    "        self.score is the output of the final layer before softmax\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.score = self.compute_product(self.activation[len(self.hidden_layers_size)],\n",
    "                                          self.weights[len(self.hidden_layers_size)])\n",
    "        \n",
    "        self.final_score = self.softmax_layer(self.score) # After softmax layer\n",
    "        \n",
    "        \n",
    "    def cross_entropy(self,y,y_):\n",
    "        return np.mean(-np.sum(y*y_, axis =1), axis = 0)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        return np.mean(np.equal(np.argmax(self.y_train_batch,axis=1),np.argmax(self.final_score, axis = 1)))\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network(train,width,[50],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN.forward_propagate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, 8, 7, 5, 5, 8, 7, 8, 4, 0, 8, 8, 8, 4, 3, 8, 7, 7, 8, 8, 4,\n",
       "       8, 8, 8, 0, 0, 4, 8, 7, 8, 7, 8, 8, 4, 0, 4, 8, 4, 8, 5, 0, 9, 8, 5,\n",
       "       8, 0, 0, 7, 8, 5, 8, 5, 8, 8, 8, 8, 8, 8, 4, 8, 7, 4, 4, 0, 8, 7, 8,\n",
       "       8, 9, 8, 8, 3, 8, 5, 3, 8, 8, 8, 9, 0, 8, 3, 8, 8, 0, 5, 7, 8, 8, 8,\n",
       "       4, 8, 8, 7, 5, 8, 8, 0])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(NN.final_score, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 3, 4, 3, 9, 9, 9, 9, 9, 9, 3, 1, 1, 4, 0, 7, 2, 7, 4, 6, 8, 2, 0,\n",
       "       8, 1, 0, 8, 9, 9, 8, 4, 6, 6, 7, 4, 3, 3, 2, 1, 8, 1, 9, 2, 5, 0, 3,\n",
       "       0, 1, 3, 1, 1, 7, 1, 7, 8, 6, 7, 2, 8, 4, 7, 0, 9, 7, 7, 1, 0, 0, 5,\n",
       "       2, 4, 1, 5, 3, 4, 9, 7, 5, 5, 4, 6, 4, 5, 2, 2, 3, 3, 3, 7, 2, 5, 1,\n",
       "       9, 4, 5, 9, 4, 8, 0, 1])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(NN.y_train_batch, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [carnd-term1]",
   "language": "python",
   "name": "Python [carnd-term1]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
